I"D-<h1 id="---coding-utf-8---">-<em>- coding: utf-8 -</em>-</h1>
<p>”"”Day #24 - Sentiment Analysis for women’s clothing E-commerce platform.ipynb</p>

<p>Automatically generated by Colaboratory.</p>

<p>Original file is located at
    https://colab.research.google.com/drive/14daGKbJn8oBZ-Afnerxgzswgai5EN6bt</p>

<h3 id="name--mohan-s">Name : Mohan S.</h3>
<h3 id="date--4821">Date : 4/8/21</h3>
<h3 id="topic--day-24-analyse-sentiments-for-tripadvisor-hotels---part-2">Topic : Day #24 Analyse Sentiments for TripAdvisor Hotels - Part 2.</h3>
<p><strong>Sentiment Analysis for women’s clothing E-commerce platform.</strong></p>

<p><strong>Problem Statement:</strong></p>

<p>In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models.
Using OSEMN framework.
“””</p>

<h1 id="import-libraries">import libraries</h1>
<p>import pandas as pd
import numpy as np
import seaborn as sns
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import spacy
import nltk</p>

<p>from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.metrics.pairwise import cosine_similarity
nltk.download(‘vader_lexicon’)
nltk.download(‘stopwords’)</p>

<p>”””# <strong>OBTAIN</strong>”””</p>

<h1 id="read-data--inspect-it">read data &amp; inspect it</h1>

<p>df = pd.read_csv(‘/content/drive/MyDrive/Data Science Uncut Bootcamp/Womens Clothing E-Commerce Reviews.csv’)
df.head()</p>

<p>df.shape</p>

<p>”””# <strong>SCRUB</strong>”””</p>

<p>df.isnull().sum()</p>

<p>#Checks the rating values in case there is a weird value
df[“Rating”].value_counts()</p>

<p>#Checks for empty review strings
df.loc[df[“Review Text”] == “”]</p>

<h1 id="delete-missing-observations-for-following-variables">Delete missing observations for following variables</h1>
<p>for x in [“Division Name”,”Department Name”,”Class Name”,”Review Text”]:
    df = df[df[x].notnull()]</p>

<h1 id="segregating-and-encoding-positive-neutral-and-negative-labels">Segregating and Encoding Positive, Neutral and Negative labels</h1>

<p>pos = [5]
neg = [1, 2]
neu = [3, 4]</p>

<p>def sentiment(rating):
  if rating in pos:
    return 2
  elif rating in neg:
    return 0
  else:
    return 1<br />
df[‘Sentiment’] = df[‘Rating’].apply(sentiment)
df.head()</p>

<h1 id="plot-chart-to-see-the-positiveneutralnegative-ratings">plot chart to see the Positive/Neutral/Negative ratings.</h1>

<p>sns.set(rc={‘figure.figsize’:(10,8)})
sns.countplot(data=df, x=’Sentiment’, color=’palegreen’)</p>

<p>”””# <strong>EXPLORE (EDA)</strong>”””</p>

<h1 id="stopwords-removal">Stopwords Removal.</h1>
<p>from nltk.corpus import stopwords
stopwords_list = set(stopwords.words(“english”))</p>

<p>#List of punctuation to remove
punctuations = “””!()-![]{};:,+’”\,&lt;&gt;./?@#$%^&amp;*_~Â”””</p>

<p>def reviewParse(review):
    #Split the review into words<br />
    splitReview = review.split()
    #Takes the stubborn punctuation out
    parsedReview = “ “.join([word.translate(str.maketrans(‘’, ‘’, punctuations)) + “ “ for word in splitReview]) 
    #Returns the parsed review
    return parsedReview #Returns the parsed review</p>

<p>def clean_review(review):
    clean_words = []
    splitReview = review.split()
    for w in splitReview:
        if w.isalpha() and w not in stopwords_list:
            clean_words.append(w.lower())
    clean_review = “ “.join(clean_words)
    return clean_review</p>

<p>#Parse all the reviews for their punctuation and add it into a new column
df[“Review Text”] = df[“Review Text”].apply(reviewParse).apply(clean_review)</p>

<p>#Take a peek at the dataset
df.head()</p>

<h1 id="keyword-used-in-different-ratings">Keyword used in different ratings</h1>
<h1 id="text-mining---visualization-type-wordcloud">Text mining - Visualization Type (wordCloud)</h1>

<p>import matplotlib.pyplot as plt
from wordcloud import WordCloud</p>

<p>def wordcloud_generator(data, title=None):
    wordcloud = WordCloud(width = 800, height = 800,
                          background_color =’royalblue’,
                          min_font_size = 10
                         ).generate(“ “.join(data.values))</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># plot the WordCloud image                        
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud, interpolation='bilinear') 
plt.axis("off") 
plt.tight_layout(pad = 0) 
plt.title(title,fontsize=30)
plt.show()
</code></pre></div></div>

<p>””“<strong>Review for all Ratings</strong>”””</p>

<h1 id="reviews-for-all-ratings">Reviews for all ratings!</h1>

<p>wordcloud_generator(df[‘Review Text’], title=”Most used words in reviews”)</p>

<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>

<p>tfidf = TfidfVectorizer(stop_words=’english’)
feature = tfidf.fit_transform(df[‘Review Text’])</p>

<h1 id="check-keyword">Check Keyword</h1>
<p>query = “fabric”
queryTFIDF = tfidf.transform([query])</p>

<h1 id="cosine-similarity">cosine similarity</h1>
<p>cosims = cosine_similarity(queryTFIDF, feature).flatten()
results = cosims.argsort()[:-6:-1]</p>

<p>for r in results:
  print(df.iloc[r][‘Review Text’])
  print(“—-“)</p>

<p>#sentiment count
df[‘Sentiment’].value_counts()</p>

<p>”"”About 10% out of whole reviews are Negative sentiments, the data looks imbalance.</p>

<h1 id="modelling"><strong>MODELLING</strong></h1>
<p>”””</p>

<p>docs = list(df[‘Review Text’])</p>

<h1 id="creating-tfidf-matrix">Creating TFIDF matrix</h1>
<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>

<h1 id="settings-that-you-use-for-count-vectorizer-will-go-here">settings that you use for count vectorizer will go here</h1>
<p>tfidf_vectorizer=TfidfVectorizer(use_idf=True)</p>

<h1 id="just-send-in-all-your-docs-here">just send in all your docs here</h1>
<p>tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)</p>

<p>X = tfidf_vectorizer_vectors.toarray()
Y = df[‘Sentiment’]
len(X[0])</p>

<h1 id="divide-the-data-into-training-and-testing-sets">Divide the data into training and testing sets</h1>
<p>from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV 
from sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, classification_report, roc_auc_score,roc_curve,auc
from sklearn.tree import DecisionTreeClassifier</p>

<p>SEED=123</p>

<p>X = tfidf_vectorizer_vectors.toarray()
Y = df[‘Sentiment’]</p>

<p>X_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.2, random_state=SEED, stratify=Y)</p>

<p>”””### 1.Decision Tree Classifier.”””</p>

<p>dt = DecisionTreeClassifier(random_state=SEED)
dt.fit(X_train,y_train)
y_pred_test = dt.predict(X_test)
print(“Training Accuracy score: “+str(round(accuracy_score(y_train,dt.predict(X_train)),4)))
print(“Testing Accuracy score: “+str(round(accuracy_score(y_test,dt.predict(X_test)),4)))</p>

<p>print(classification_report(y_test, y_pred_test, target_names=[‘positive’, ‘neutral’, ‘negative’]))</p>

<p>cm = confusion_matrix(y_test, y_pred_test)
#print(‘Confusion matrix\n’, cm)
cm_matrix = pd.DataFrame(data=cm, columns=[‘Actual Negative’, ‘Actual Neutral’, ‘Actual Positive’], 
                        index=[‘Predict Negative’, ‘Predict Neutral’, ‘Predict Positive’])
sns.heatmap(cm_matrix, annot=True, fmt=’d’, cmap=’YlGnBu’)
plt.show()</p>

<p>”””### 2. Naive Bayes Classifier”””</p>

<p>from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred_train = gnb.predict(X_train)
y_pred_test = gnb.predict(X_test)
print(“Training Accuracy score: “+str(round(accuracy_score(y_train,gnb.predict(X_train)),4)))
print(“Testing Accuracy score: “+str(round(accuracy_score(y_test,gnb.predict(X_test)),4)))</p>

<p>print(classification_report(y_test, y_pred_test, target_names=[‘positive’, ‘neutral’, ‘negative’]))</p>

<p>cm = confusion_matrix(y_test, y_pred_test)
#print(‘Confusion matrix\n’, cm)
cm_matrix = pd.DataFrame(data=cm, columns=[‘Actual Negative’, ‘Actual Neutral’, ‘Actual Positive’], 
                        index=[‘Predict Negative’, ‘Predict Neutral’, ‘Predict Positive’])
sns.heatmap(cm_matrix, annot=True, fmt=’d’, cmap=’YlGnBu’)
plt.show()</p>

<p>”””### 3.Logistic Regression”””</p>

<p>from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=SEED).fit(X_train, y_train)
y_pred_train = lr.predict(X_train)
y_pred_test = lr.predict(X_test)
print(“Training Accuracy score: “+str(round(accuracy_score(y_train,lr.predict(X_train)),4)))
print(“Testing Accuracy score: “+str(round(accuracy_score(y_test,lr.predict(X_test)),4)))</p>

<p>print(classification_report(y_test, y_pred_test, target_names=[‘positive’, ‘neutral’, ‘negative’]))</p>

<p>cm = confusion_matrix(y_test, y_pred_test)
#print(‘Confusion matrix\n’, cm)
cm_matrix = pd.DataFrame(data=cm, columns=[‘Actual Negative’, ‘Actual Neutral’, ‘Actual Positive’], 
                        index=[‘Predict Negative’, ‘Predict Neutral’, ‘Predict Positive’])
sns.heatmap(cm_matrix, annot=True, fmt=’d’, cmap=’YlGnBu’)
plt.show()</p>

<p>”””### 4. Random Forest Classifier”””</p>

<p>from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
print(“Training Accuracy score: “+str(round(accuracy_score(y_train,clf.predict(X_train)),4)))
print(“Testing Accuracy score: “+str(round(accuracy_score(y_test,clf.predict(X_test)),4)))</p>

<p>print(classification_report(y_test, y_pred_test, target_names=[‘positive’, ‘neutral’, ‘negative’]))</p>

<p>cm = confusion_matrix(y_test, y_pred_test)
#print(‘Confusion matrix\n’, cm)
cm_matrix = pd.DataFrame(data=cm, columns=[‘Actual Negative’, ‘Actual Neutral’, ‘Actual Positive’], 
                        index=[‘Predict Negative’, ‘Predict Neutral’, ‘Predict Positive’])
sns.heatmap(cm_matrix, annot=True, fmt=’d’, cmap=’YlGnBu’)
plt.show()</p>

<p>”””## <strong>My Findings:</strong></p>

<p>Using the TF-IDF technique, analysing “Keywords” in the reviews was not difficult. It’s finds the keywords in the reviews pretty fine. However, in the overall reviews, we have very less negative reviews compared to positive reviews, it’s looked like an im-balanced data.</p>

<p>We have reviews with 5 star but the compound score is negative. In contrast, we have a 1 star review with compound score is high. I suppose we cant analyze the review’s sentiment by just looking at the computed normalizing value for the polarity scores (negative,neutral and positive).</p>

<p>Though it may seem easy, Sentiment Analysis is a tricky subject. A text may contain multiple sentiments all at once. Text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion.</p>

<p>I guess the data need to be cleaned even further and the types of analysis to perform on the models. This will need to be used to measure the attitude, sentiments, evaluations, and emotions of a writer or reviewer.
“””</p>
:ET